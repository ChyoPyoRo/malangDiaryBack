from pydantic import BaseModel
import numpy as np
import pandas as pd
from numpy import dot
from numpy.linalg import norm
import urllib.request
# from sentence_transformers import SentenceTransformer
from typing import Optional, List
###
import re
import pickle
from keras.models import load_model
from konlpy.tag import Okt
# from keras.preprocessing.text import Tokenizer


class latestContent(BaseModel):
    content: str


class dataType(BaseModel):
    input: str
    vector: List[str] = []


class model:
    # def sentenceSimilarity(data: dataType):
    #     print(data)
    #     model = SentenceTransformer(
    #         'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')
    #     rawData = data.input
    #     currentContent = model.encode(rawData)
    #     compareContentString = data.vector
    #     simList = []
    #     # print("ğŸ”¥ğŸ”¥1")
    #     def tofloat(data):
    #         data = data.replace("[", "").replace(
    #             "]", "").replace("\n", "").split(" ")
    #         data = [float(i) for i in data if i != '']
    #         return data
    #     for vec in compareContentString:
    #         result = np.array(tofloat(vec))
    #         simList.append(result)

    #     def cos_sim(a, b):
    #         return dot(a, b)/(norm(a)*norm(b))

    #     simDic = {}
    #     for i in range(len(simList)-3):
    #         result = cos_sim(currentContent, simList[i])
    #         simDic[i] = result
    #     # print("ğŸ”¥ğŸ”¥2")

    #     sortedDic = sorted(
    #         simDic.items(), key=lambda item: item[1], reverse=True)
    #     resultList = []
    #     for i in range(3):
    #         resultList.append(sortedDic[i][0])
    #     print("ğŸ”¥ğŸ”¥3")
    #     print("sim dic í˜•íƒœí™•ì¸ìš© ", resultList)

    #     currentContentVec = np.array2string(currentContent)
    #     returnValue = {"result":resultList, "vector":currentContentVec}
    #     # returnValue = {"result":"ì„ì‹œ", "vector":currentContentVec}
    #     return returnValue

    # def sentenceSimilarityUpdate(data: latestContent):
    #     model = SentenceTransformer(
    #         'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')
    #     rawData = data.content
    #     currentContent = model.encode(rawData)

    #     currentContentVec = np.array2string(currentContent)
    #     # returnValue = {"vector":currentContentVec}
    #     return currentContentVec

    def emotionAnalysis(content: latestContent):
        print("model!!!",content)
        model = load_model('team10_roberta_word_1212.h5')
        print("model ëª» ë¶ˆëŸ¬ì˜¤ëŠ”ê±° ê°™ì€ë°?",model)

        def text_cleaning(content):
            text = content.content
            print("model textcleaning","ğŸ‘¾")

            okt = Okt()
            words = okt.pos(text, stem=True)
            print("model textcleaning","ğŸ‘¾ğŸ‘¾")
            words_avn = [word[0] for word in words if word[1] ==
                         'Adjective' or word[1] == 'Verb' or word[1] == 'Noun']

            print("model words_avn","ğŸ‘¾ğŸ‘¾ğŸ‘¾")
            return words_avn

        with open('./tokenizer_1212.pkl', 'rb') as tk:
            print("tokenizer","ğŸ‘¾ğŸ‘¾ğŸ‘¾ğŸ‘¾")
            tokenizer = pickle.load(tk)

        word_index = tokenizer.word_index

        X = tokenizer.texts_to_sequences(text_cleaning(content))

        print("tokenizer","ğŸ¥")
        prediction = np.array(model.predict(X))
        result = prediction.sum(axis=0)

        idx = np.argmax(result)
        print("idx","ğŸ¥", idx)
        emotion_dict = {0: 'ê°ì‚¬í•œ', 1: 'ì‹ ì´ ë‚œ', 2: 'ìì‹ ê°',
                        3: 'í¸ì•ˆí•œ', 4: 'ë¶„ë…¸', 5: 'ë¶ˆì•ˆ', 6: 'ìƒì²˜', 7: 'ìŠ¬í””'}
        print("model ì—¬ê¸° ì•ˆì˜¤ë‚˜?")

        return emotion_dict[idx]

# print(sentenceSimilarity("ë°°ê³ í”„ê³  ì¡¸ë ¤"))
